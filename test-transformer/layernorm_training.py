"""
Show how LayerNorm parameters Œ≥ and Œ≤ are learned during training
"""

import torch
import torch.nn as nn
import torch.optim as optim


def show_layernorm_learning():
    """Demonstrate that Œ≥ and Œ≤ are trainable parameters"""
    print("=" * 50)
    print("üéì LAYERNORM PARAMETERS ARE LEARNED!")
    print("=" * 50)

    # Create a LayerNorm layer
    d_model = 4
    layer_norm = nn.LayerNorm(d_model)

    print(f"üîß LayerNorm layer created with d_model = {d_model}")

    # Show initial parameters
    print(f"\nüìä Initial parameters:")
    print(f"   Œ≥ (weight): {layer_norm.weight}")
    print(f"   Œ≤ (bias):   {layer_norm.bias}")
    print(f"   ‚Ä¢ Œ≥ initialized to all 1s (vector with {d_model} elements)")
    print(f"   ‚Ä¢ Œ≤ initialized to all 0s (vector with {d_model} elements)")
    print(f"   ‚Ä¢ NOT scalars! Each dimension has its own Œ≥ and Œ≤")

    print(f"\nüî¢ Parameter shapes:")
    print(f"   Œ≥ shape: {layer_norm.weight.shape}")
    print(f"   Œ≤ shape: {layer_norm.bias.shape}")
    print(
        f"   Total learnable parameters: {layer_norm.weight.numel() + layer_norm.bias.numel()}"
    )

    # Check if they require gradients
    print(f"\nüéØ Are they trainable?")
    print(f"   Œ≥ requires_grad: {layer_norm.weight.requires_grad}")
    print(f"   Œ≤ requires_grad: {layer_norm.bias.requires_grad}")

    # Show they have gradients after backward pass
    input_data = torch.randn(2, 4)  # batch_size=2, d_model=4
    output = layer_norm(input_data)
    loss = output.sum()  # Dummy loss
    loss.backward()

    print(f"\nüîÑ After backward pass:")
    print(f"   Œ≥ gradients: {layer_norm.weight.grad}")
    print(f"   Œ≤ gradients: {layer_norm.bias.grad}")
    print(f"   ‚Ä¢ Each dimension gets its own gradient!")
    print(f"   ‚Ä¢ Gradients are computed - they can be updated!")


def simulate_training():
    """Simulate how parameters change during training"""
    print(f"\n" + "=" * 50)
    print("üèÉ SIMULATE TRAINING - WATCH PARAMETERS CHANGE")
    print("=" * 50)

    # Simple setup
    d_model = 3
    layer_norm = nn.LayerNorm(d_model)
    optimizer = optim.SGD(layer_norm.parameters(), lr=0.1)

    print(f"üìä Training setup:")
    print(f"   d_model: {d_model}")
    print(f"   Learning rate: 0.1")
    print(f"   Optimizer: SGD")

    # Show parameter evolution
    print(f"\nüìà Parameter evolution during training:")

    for epoch in range(5):
        # Forward pass
        input_data = torch.randn(1, d_model)
        output = layer_norm(input_data)

        # Dummy loss (just sum of outputs)
        loss = output.sum()

        print(f"\n   Epoch {epoch}:")
        print(f"   Œ≥: {layer_norm.weight.data}")
        print(f"   Œ≤: {layer_norm.bias.data}")

        # Backward pass and update
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"\n‚úÖ Notice how Œ≥ and Œ≤ changed from their initial values!")
    print(f"   ‚Ä¢ They learned to scale and shift the normalized values")
    print(f"   ‚Ä¢ This helps the model adapt to the specific data distribution")


def scalar_vs_vector_demo():
    """Show clearly which are scalars vs vectors"""
    print(f"\n" + "=" * 50)
    print("üìè SCALAR vs VECTOR IN LAYER NORMALIZATION")
    print("=" * 50)

    # Example with d_model = 4
    x = torch.tensor([10.0, 2.0, 30.0, 5.0])
    d_model = len(x)

    print(f"üìä Input vector: {x}")
    print(f"   d_model = {d_model}")

    # Calculate mean and std (scalars)
    mu = x.mean()
    sigma = x.std()

    print(f"\nüî¢ Statistics (SCALARS - single numbers):")
    print(f"   Œº (mean) = sum({x}) / {d_model} = {mu:.2f}")
    print(f"   œÉ (std) = {sigma:.2f}")
    print(f"   ‚Ä¢ Œº and œÉ are computed from ALL dimensions ‚Üí scalars")

    # Normalize
    normalized = (x - mu) / sigma

    print(f"\n‚ú® After normalization: {normalized}")
    print(f"   All elements normalized using the SAME Œº and œÉ")

    # Layer norm parameters (vectors)
    gamma = torch.tensor([2.0, 0.5, 1.0, 3.0])  # One per dimension
    beta = torch.tensor([0.1, -0.2, 0.0, 0.5])  # One per dimension

    print(f"\nüéØ Learnable parameters (VECTORS - one per dimension):")
    print(f"   Œ≥ = {gamma} (shape: {gamma.shape})")
    print(f"   Œ≤ = {beta} (shape: {beta.shape})")
    print(f"   ‚Ä¢ Each dimension gets its own Œ≥ and Œ≤")

    # Final result
    final = gamma * normalized + beta

    print(f"\nüìà Element-wise calculation:")
    for i in range(d_model):
        print(
            f"   dim {i}: {gamma[i]:.1f} * {normalized[i]:.2f} + {beta[i]:.1f} = {final[i]:.2f}"
        )

    print(f"\n‚úÖ Final output: {final}")

    print(f"\nüí° Summary:")
    print(f"   ‚Ä¢ Œº, œÉ: SCALARS (1 number each)")
    print(f"   ‚Ä¢ Œ≥, Œ≤: VECTORS ({d_model} numbers each)")
    print(f"   ‚Ä¢ Total learnable params: {len(gamma) + len(beta)}")


def why_learnable_parameters():
    """Explain why Œ≥ and Œ≤ need to be learnable"""
    print(f"\n" + "=" * 50)
    print("‚ùì WHY ARE Œ≥ AND Œ≤ LEARNABLE?")
    print("=" * 50)

    print(f"\nü§î Without learnable parameters:")
    print(f"   ‚Ä¢ Normalization always forces mean=0, std=1")
    print(f"   ‚Ä¢ But maybe the model needs different scales per dimension")
    print(f"   ‚Ä¢ The model has no way to adjust this!")

    print(f"\n‚úÖ With learnable Œ≥ and Œ≤:")
    print(f"   ‚Ä¢ Model can learn optimal scale (Œ≥) and shift (Œ≤) per dimension")
    print(f"   ‚Ä¢ Example: Œ≥=[2.0, 0.5, 1.5], Œ≤=[0.1, -0.3, 0.8]")
    print(f"   ‚Ä¢ Each feature dimension can have its own normalization!")

    print(f"\nüß† Why vectors not scalars?")
    print(f"   ‚Ä¢ Different dimensions represent different features:")
    print(f"     - Dim 0: verb tense information")
    print(f"     - Dim 1: subject-object relationships")
    print(f"     - Dim 2: sentiment polarity")
    print(f"     - Dim 3: grammatical number")
    print(f"   ‚Ä¢ Each feature needs its own optimal scale/shift!")

    # Example showing the effect
    d_model = 3
    x = torch.tensor([1.0, 2.0, 3.0])

    # Standard normalization (mean=0, std=1)
    normalized = (x - x.mean()) / x.std()

    # With learned parameters
    gamma = torch.tensor([2.0, 0.5, 1.5])  # Learned scales
    beta = torch.tensor([0.1, -0.3, 0.8])  # Learned shifts
    final = gamma * normalized + beta

    print(f"\nüìä Example:")
    print(f"   Input: {x}")
    print(f"   After normalization: {normalized}")
    print(f"   With Œ≥={gamma}, Œ≤={beta}:")
    print(f"   Final output: {final}")
    print(f"   ‚Üí Model learned different transforms per dimension!")


def main():
    show_layernorm_learning()
    simulate_training()
    scalar_vs_vector_demo()
    why_learnable_parameters()

    print(f"\n" + "=" * 50)
    print(f"üéì KEY INSIGHTS")
    print(f"=" * 50)
    print(f"1. Œ≥ and Œ≤ are trainable parameters (requires_grad=True)")
    print(f"2. They start at Œ≥=1, Œ≤=0 (identity transformation)")
    print(f"3. During training, they learn optimal scale/shift values")
    print(f"4. This allows the model to adapt normalization to its needs")
    print(f"5. Without them, normalization would be too restrictive")
    print(f"6. They're updated by the optimizer just like other weights!")


if __name__ == "__main__":
    main()
