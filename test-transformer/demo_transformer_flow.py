"""
Demo script to visualize the input/output flow in transformer training and inference
Shows the teacher forcing mechanism and autoregressive generation clearly
"""

import torch
import torch.nn as nn


def visualize_teacher_forcing():
    """Demonstrate teacher forcing during training"""
    print("\n" + "=" * 70)
    print("ğŸ“ TEACHER FORCING (Training Mode)")
    print("=" * 70)

    # Example sentence: "I love you"
    print("\nğŸ“š Training Example: English â†’ Italian")
    print("   Source: 'I love you' â†’ 'ti amo'")

    # Tokenized version
    src_tokens = ["<sos>", "I", "love", "you", "<eos>"]
    tgt_tokens = ["<sos>", "ti", "amo", "<eos>"]

    print(f"\nğŸ”¤ Tokenized:")
    print(f"   Source: {' '.join(src_tokens)}")
    print(f"   Target: {' '.join(tgt_tokens)}")

    # Teacher forcing steps
    print("\nğŸ“– Teacher Forcing Steps:")
    print("   The model learns to predict the next token given previous tokens")
    print("   " + "-" * 60)

    for i in range(len(tgt_tokens) - 1):
        input_seq = tgt_tokens[: i + 1]
        target_token = tgt_tokens[i + 1]

        print(f"\n   Step {i+1}:")
        print(f"   â¡ï¸  Input:  {' '.join(input_seq):20s} ")
        print(f"   ğŸ¯ Target: {target_token:20s}")

        if i == 0:
            print(f"      (Given '<sos>', predict 'ti')")
        elif i == 1:
            print(f"      (Given '<sos> ti', predict 'amo')")
        elif i == 2:
            print(f"      (Given '<sos> ti amo', predict '<eos>')")

    print("\n   " + "-" * 60)
    print("   ğŸ’¡ All predictions happen in parallel during training!")
    print("   The decoder sees: '<sos> ti amo' (shifted right)")
    print("   And predicts:     'ti amo <eos>' (original sequence)")


def visualize_autoregressive_generation():
    """Demonstrate autoregressive generation during inference"""
    print("\n" + "=" * 70)
    print("ğŸ¤– AUTOREGRESSIVE GENERATION (Inference Mode)")
    print("=" * 70)

    print("\nğŸ“š Translation Example: English â†’ Italian")
    print("   Source: 'I love you'")
    print("   Goal: Generate 'ti amo'")

    # Source tokens
    src_tokens = ["<sos>", "I", "love", "you", "<eos>"]
    print(f"\nğŸ”¤ Source (encoded): {' '.join(src_tokens)}")

    # Generation process
    print("\nğŸ“– Generation Steps (one token at a time):")
    print("   " + "-" * 60)

    generated = ["<sos>"]
    predictions = ["ti", "amo", "<eos>"]

    for i, next_token in enumerate(predictions):
        print(f"\n   Step {i+1}:")
        print(f"   ğŸ“¥ Decoder Input:  {' '.join(generated):25s}")
        print(f"   ğŸ”® Model Predicts: {next_token:25s}")

        generated.append(next_token)

        print(f"   ğŸ“¤ Updated Sequence: {' '.join(generated)}")

        if next_token == "<eos>":
            print(f"   âœ… End of sequence detected - stop generation")
            break
        else:
            print(f"   â†»  Feed back into decoder for next prediction...")

    print("\n   " + "-" * 60)
    print(f"   ğŸ‰ Final Translation: {' '.join(generated[1:-1])}")


def visualize_attention_mechanism():
    """Visualize how attention works conceptually"""
    print("\n" + "=" * 70)
    print("ğŸ” ATTENTION MECHANISM")
    print("=" * 70)

    print("\nğŸ“š Example: Translating 'I love you' â†’ 'ti amo'")

    print("\n1ï¸âƒ£  Self-Attention in Encoder:")
    print("   Each English word attends to all other English words")
    print("   'love' might strongly attend to 'I' and 'you' for context")

    print("\n2ï¸âƒ£  Masked Self-Attention in Decoder:")
    print("   Each Italian word can only attend to previous Italian words")
    print("   'amo' can attend to '<sos>' and 'ti', but not future tokens")

    print("\n3ï¸âƒ£  Cross-Attention (Encoder-Decoder):")
    print("   Each Italian word attends to all English words")
    print("   'ti' might attend strongly to 'you'")
    print("   'amo' might attend strongly to 'love'")

    # Simple attention visualization
    print("\nğŸ“Š Simplified Attention Matrix (Cross-Attention):")
    print("        I    love   you")
    print("   ti   0.1   0.2   0.7  â†’ 'ti' focuses on 'you'")
    print("   amo  0.2   0.7   0.1  â†’ 'amo' focuses on 'love'")


def main():
    print("\n" + "ğŸŒŸ" * 35)
    print("  TRANSFORMER MODEL: INPUT/OUTPUT FLOW VISUALIZATION")
    print("ğŸŒŸ" * 35)

    # Show teacher forcing
    visualize_teacher_forcing()

    # Show autoregressive generation
    visualize_autoregressive_generation()

    # Show attention concept
    visualize_attention_mechanism()

    print("\n" + "=" * 70)
    print("ğŸ“ KEY DIFFERENCES:")
    print("=" * 70)
    print("\nğŸ“ Training (Teacher Forcing):")
    print("   â€¢ All predictions made in parallel")
    print("   â€¢ Model sees correct previous tokens")
    print("   â€¢ Fast and efficient")
    print("   â€¢ Input: '<sos> ti amo' â†’ Output: 'ti amo <eos>'")

    print("\nğŸ¤– Inference (Autoregressive):")
    print("   â€¢ Generate one token at a time")
    print("   â€¢ Model sees its own predictions")
    print("   â€¢ Slower but necessary for generation")
    print("   â€¢ Start with '<sos>', generate until '<eos>'")

    print("\n" + "ğŸŒŸ" * 35)


if __name__ == "__main__":
    main()
