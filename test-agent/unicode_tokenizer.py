import random
import secrets
import tiktoken
from typing import List, Tuple


def analyze_tokenization(text: str) -> Tuple[List[int], List[str], int]:
    """
    Analyze how GPT tokenizes the given text using cl100k_base encoding
    Returns: (token_ids, token_strings, token_count)
    """
    # Get the cl100k_base encoding (used by GPT-4 and GPT-3.5-turbo)
    encoding = tiktoken.get_encoding("cl100k_base")

    # Encode the text to get token IDs
    token_ids = encoding.encode(text)

    # Decode each token ID to see the actual token strings
    token_strings = [encoding.decode([token_id]) for token_id in token_ids]

    # Count tokens
    token_count = len(token_ids)

    return token_ids, token_strings, token_count


def display_tokenization_analysis(text: str, description: str = ""):
    """Display detailed tokenization analysis for given text"""
    print(f"\n{'='*70}")
    print(f"TOKENIZATION ANALYSIS: {description}")
    print(f"{'='*70}")
    print(f"Original text: {text}")
    print(f"Text length: {len(text)} characters")
    print(f"Byte length: {len(text.encode('utf-8'))} bytes")

    try:
        token_ids, token_strings, token_count = analyze_tokenization(text)

        print(f"\nğŸ¤– Encoding: cl100k_base (GPT-4/GPT-3.5-turbo)")
        print(f"ğŸ“Š Token count: {token_count}")
        print(f"ğŸ’° Compression ratio: {len(text)/token_count:.2f} chars per token")
        print(
            f"ğŸ—œï¸  Byte compression: {len(text.encode('utf-8'))/token_count:.2f} bytes per token"
        )

        print(f"\nğŸ”¤ Token breakdown:")
        for i, (token_id, token_str) in enumerate(zip(token_ids, token_strings)):
            # Show token with visual boundaries and escape special chars
            token_display = repr(token_str)
            byte_len = len(token_str.encode("utf-8"))
            print(f"  [{i+1:2d}] ID:{token_id:5d} ({byte_len:2d}b) â†’ {token_display}")

        print(f"\nğŸ“ Reconstructed: {''.join(token_strings)}")

    except Exception as e:
        print(f"âŒ Error: {e}")


def analyze_languages():
    """Analyze tokenization of different languages"""

    languages = {
        "English": "Hello, how are you today?",
        "Spanish": "Â¡Hola! Â¿CÃ³mo estÃ¡s hoy?",
        "French": "Bonjour ! Comment allez-vous aujourd'hui ?",
        "German": "Hallo! Wie geht es Ihnen heute?",
        "Chinese (Simplified)": "ä½ å¥½ï¼ä½ ä»Šå¤©å¥½å—ï¼Ÿ",
        "Chinese (Traditional)": "ä½ å¥½ï¼ä½ ä»Šå¤©å¥½å—ï¼Ÿ",
        "Japanese": "ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã¯ã„ã‹ãŒã§ã™ã‹ï¼Ÿ",
        "Korean": "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ì–´ë– ì„¸ìš”?",
        "Russian": "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! ĞšĞ°Ğº Ğ´ĞµĞ»Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ?",
        "Arabic": "Ù…Ø±Ø­Ø¨Ø§! ÙƒÙŠÙ Ø­Ø§Ù„Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
        "Hindi": "à¤¨à¤®à¤¸à¥à¤¤à¥‡! à¤†à¤œ à¤†à¤ª à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚?",
        "Thai": "à¸ªà¸§à¸±à¸ªà¸”à¸µ! à¸§à¸±à¸™à¸™à¸µà¹‰à¹€à¸›à¹‡à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸šà¹‰à¸²à¸‡?",
        "Hebrew": "×©×œ×•×! ××™×š ××ª×” ×”×™×•×?",
        "Greek": "Î“ÎµÎ¹Î± ÏƒÎ±Ï‚! Î ÏÏ‚ ÎµÎ¯ÏƒÏ„Îµ ÏƒÎ®Î¼ÎµÏÎ±;",
    }

    print("ğŸŒ UNICODE LANGUAGE TOKENIZATION ANALYSIS")
    print("=" * 70)
    print("This shows how GPT tokenizes different languages")
    print("Understanding cross-language tokenization is crucial for multilingual AI!\n")

    # Analyze each language
    for language, text in languages.items():
        display_tokenization_analysis(text, f"{language}")

    # Summary comparison
    print(f"\n{'='*70}")
    print("LANGUAGE EFFICIENCY COMPARISON")
    print(f"{'='*70}")
    print(
        f"{'Language':<20} | {'Chars':<5} | {'Bytes':<5} | {'Tokens':<6} | {'C/T':<4} | {'B/T':<4}"
    )
    print("-" * 70)

    for language, text in languages.items():
        _, _, token_count = analyze_tokenization(text)
        chars = len(text)
        bytes_len = len(text.encode("utf-8"))
        chars_per_token = chars / token_count
        bytes_per_token = bytes_len / token_count

        print(
            f"{language:<20} | {chars:<5} | {bytes_len:<5} | {token_count:<6} | {chars_per_token:<4.1f} | {bytes_per_token:<4.1f}"
        )


def analyze_emojis():
    """Analyze tokenization of emojis and special Unicode characters"""

    emoji_tests = {
        "Simple Emojis": "ğŸ˜€ğŸ˜‚ğŸ¥°ğŸ˜ğŸ¤”",
        "Complex Emojis": "ğŸ‘¨â€ğŸ’»ğŸ‘©â€ğŸš€ğŸ§‘â€ğŸ¨ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦",
        "Flag Emojis": "ğŸ‡ºğŸ‡¸ğŸ‡¯ğŸ‡µğŸ‡©ğŸ‡ªğŸ‡«ğŸ‡·ğŸ‡¨ğŸ‡³",
        "Mixed Text + Emojis": "Hello! ğŸ˜Š How are you? ğŸ¤” Have a great day! ğŸŒŸ",
        "Emoji Sequences": "ğŸ‰ğŸŠğŸˆğŸğŸ‚",
        "Skin Tone Modifiers": "ğŸ‘‹ğŸ»ğŸ‘‹ğŸ¼ğŸ‘‹ğŸ½ğŸ‘‹ğŸ¾ğŸ‘‹ğŸ¿",
        "Gender Variants": "ğŸ§‘â€ğŸ’¼ğŸ‘¨â€ğŸ’¼ğŸ‘©â€ğŸ’¼",
        "Animal Emojis": "ğŸ¶ğŸ±ğŸ­ğŸ¹ğŸ°ğŸ¦ŠğŸ»ğŸ¼",
    }

    print(f"\n{'='*70}")
    print("EMOJI & SPECIAL UNICODE ANALYSIS")
    print(f"{'='*70}")

    for category, text in emoji_tests.items():
        display_tokenization_analysis(text, f"Emojis: {category}")


def analyze_special_unicode():
    """Analyze special Unicode characters and symbols"""

    special_tests = {
        "Mathematical Symbols": "âˆ‘âˆ«âˆ‚âˆ†âˆ‡âˆšâˆâ‰ˆâ‰ â‰¤â‰¥Â±Ã—Ã·",
        "Currency Symbols": "$ â‚¬ Â£ Â¥ â‚¹ â‚½ â‚¿ Â¢ â‚© â‚ª",
        "Arrows": "â†’â†â†‘â†“â†”â†•â‡’â‡â‡‘â‡“â‡”â‡•",
        "Box Drawing": "â”Œâ”€â”â”‚â””â”€â”˜â”¬â”´â”œâ”¤â”¼",
        "Braille": "â â ƒâ ‰â ™â ‘â ‹â ›â “â Šâ šâ …â ‡â â â •â â Ÿâ —â â â ¥â §â ºâ ­â ½â µ",
        "Diacritics": "Ã Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã±Ã²Ã³Ã´ÃµÃ¶Ã¸Ã¹ÃºÃ»Ã¼Ã½",
        "Superscripts": "Â¹Â²Â³â´âµâ¶â·â¸â¹â°áµƒáµ‡á¶œáµˆáµ‰á¶ áµÊ°â±Ê²áµË¡áµâ¿áµ’áµ–Ê³Ë¢áµ—áµ˜áµ›Ê·Ë£Ê¸á¶»",
        "Subscripts": "â‚â‚‚â‚ƒâ‚„â‚…â‚†â‚‡â‚ˆâ‚‰â‚€â‚â‚‘â‚•áµ¢â±¼â‚–â‚—â‚˜â‚™â‚’â‚šáµ£â‚›â‚œáµ¤áµ¥â‚“",
    }

    print(f"\n{'='*70}")
    print("SPECIAL UNICODE CHARACTERS ANALYSIS")
    print(f"{'='*70}")

    for category, text in special_tests.items():
        display_tokenization_analysis(text, f"Special: {category}")


def analyze_mixed_content():
    """Analyze mixed content with different Unicode ranges"""

    mixed_tests = {
        "Code with Unicode": "def hello_ä¸–ç•Œ(): return 'ä½ å¥½ World! ğŸŒ'",
        "Multilingual Sentence": "Hello à¤¨à¤®à¤¸à¥à¤¤à¥‡ ã“ã‚“ã«ã¡ã¯ ì•ˆë…•í•˜ì„¸ìš” Ù…Ø±Ø­Ø¨Ø§ Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹Ñ‚Ğµ",
        "Programming Symbols": "Î» x: xÂ² + âˆš(x) â‰ˆ âˆ«f(x)dx where x âˆˆ â„",
        "Social Media Text": "Just had an amazing lunch! ğŸ•ğŸŸ #foodie #yummy ğŸ˜‹ @friend",
        "Scientific Notation": "E = mcÂ² where c â‰ˆ 3Ã—10â¸ m/s and â„ = 1.054Ã—10â»Â³â´ Jâ‹…s",
        "Mixed Scripts": "Ğ ÑƒÑÑĞºĞ¸Ğ¹ English ä¸­æ–‡ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© à¤¹à¤¿à¤‚à¤¦à¥€ æ—¥æœ¬èª í•œêµ­ì–´",
        "Web Content": "Visit https://example.com for more info! ğŸ“§ contact@example.com",
        "JSON with Unicode": '{"name": "å¼ ä¸‰", "emoji": "ğŸ˜Š", "price": "Â¥100"}',
    }

    print(f"\n{'='*70}")
    print("MIXED UNICODE CONTENT ANALYSIS")
    print(f"{'='*70}")

    for category, text in mixed_tests.items():
        display_tokenization_analysis(text, f"Mixed: {category}")


def compare_encoding_efficiency():
    """Compare tokenization efficiency across different text types"""

    test_texts = {
        "ASCII Only": "The quick brown fox jumps over the lazy dog.",
        "Latin Extended": "CafÃ© naÃ¯ve rÃ©sumÃ© piÃ±ata faÃ§ade",
        "Cyrillic": "Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ ĞºĞ¾Ñ€Ğ¸Ñ‡Ğ½ĞµĞ²Ğ°Ñ Ğ»Ğ¸ÑĞ° Ğ¿Ñ€Ñ‹Ğ³Ğ°ĞµÑ‚ Ñ‡ĞµÑ€ĞµĞ· Ğ»ĞµĞ½Ğ¸Ğ²ÑƒÑ ÑĞ¾Ğ±Ğ°ĞºÑƒ",
        "Chinese": "å¿«é€Ÿçš„æ£•è‰²ç‹ç‹¸è·³è¿‡æ‡’æƒ°çš„ç‹—",
        "Japanese": "ç´ æ—©ã„èŒ¶è‰²ã®ã‚­ãƒ„ãƒãŒæ€ ã‘è€…ã®çŠ¬ã‚’é£›ã³è¶Šãˆã‚‹",
        "Arabic": "Ø§Ù„Ù‚ÙØ² Ø§Ù„Ø³Ø±ÙŠØ¹ Ø§Ù„Ø¨Ù†ÙŠ Ø§Ù„Ø«Ø¹Ù„Ø¨ ÙÙˆÙ‚ Ø§Ù„ÙƒÙ„Ø¨ Ø§Ù„ÙƒØ³ÙˆÙ„",
        "Emoji Heavy": "ğŸ¦ŠğŸƒâ€â™‚ï¸ğŸ’¨ğŸ•â€ğŸ¦ºğŸ˜´ Quick fox jumps over lazy dog! ğŸŒŸâœ¨",
        "Mixed Scripts": "The å¿«é€Ÿ ğŸ¦Š fox à¤¸à¤°à¥à¤¦à¥€ over the æ€ æƒ° ğŸ•",
    }

    print(f"\n{'='*70}")
    print("ENCODING EFFICIENCY COMPARISON")
    print(f"{'='*70}")
    print(
        f"{'Text Type':<15} | {'Chars':<5} | {'Bytes':<5} | {'Tokens':<6} | {'C/T':<4} | {'B/T':<4} | {'Efficiency'}"
    )
    print("-" * 80)

    baseline_efficiency = None

    for text_type, text in test_texts.items():
        _, _, token_count = analyze_tokenization(text)
        chars = len(text)
        bytes_len = len(text.encode("utf-8"))
        chars_per_token = chars / token_count
        bytes_per_token = bytes_len / token_count

        if baseline_efficiency is None:
            baseline_efficiency = chars_per_token

        efficiency_ratio = chars_per_token / baseline_efficiency
        efficiency_desc = (
            "Excellent"
            if efficiency_ratio > 0.9
            else (
                "Good"
                if efficiency_ratio > 0.7
                else "Fair" if efficiency_ratio > 0.5 else "Poor"
            )
        )

        print(
            f"{text_type:<15} | {chars:<5} | {bytes_len:<5} | {token_count:<6} | {chars_per_token:<4.1f} | {bytes_per_token:<4.1f} | {efficiency_desc}"
        )


def interactive_unicode_tokenizer():
    """Interactive mode to analyze custom Unicode text"""
    print(f"\n{'='*70}")
    print("INTERACTIVE UNICODE TOKENIZER")
    print(f"{'='*70}")
    print("Enter any Unicode text to see how GPT tokenizes it!")
    print("Try different languages, emojis, symbols, mixed content...")
    print("Type 'quit' to exit, 'examples' to see sample texts")

    examples = [
        "ä½ å¥½ä¸–ç•Œ! ğŸŒ",
        "Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹ Ğ¼Ğ¸Ñ€! ğŸŒ",
        "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…! ğŸŒ",
        "ğŸš€ Î»x: xÂ² + âˆ«f(x)dx â‰ˆ âˆ‘âˆ",
        "ğŸ‘¨â€ğŸ’» def hello_ä¸–ç•Œ(): return 'ğŸŒŸ'",
    ]

    while True:
        user_input = input("\nğŸ”¤ Enter Unicode text (or 'quit'/'examples'): ").strip()

        if user_input.lower() in ["quit", "exit", "q"]:
            print("ğŸ‘‹ Goodbye!")
            break
        elif user_input.lower() in ["examples", "ex", "e"]:
            print("\nğŸ“ Example texts to try:")
            for i, example in enumerate(examples, 1):
                print(f"  {i}. {example}")
            continue
        elif user_input.isdigit() and 1 <= int(user_input) <= len(examples):
            text = examples[int(user_input) - 1]
            print(f"ğŸ¯ Using example: {text}")
            display_tokenization_analysis(text, f"Example #{user_input}")
        elif user_input:
            display_tokenization_analysis(user_input, "User Input")
        else:
            print("ğŸ’¡ Enter some text to analyze!")


def main():
    """Main function to run all demonstrations"""

    print("ğŸŒ UNICODE TOKENIZATION DEMO")
    print(
        "This demonstrates how GPT tokenizes Unicode text across languages and scripts"
    )
    print(
        "Understanding Unicode tokenization is crucial for multilingual AI applications!\n"
    )

    # Run different analyses
    try:
        # 1. Language analysis
        analyze_languages()

        # 2. Emoji analysis
        analyze_emojis()

        # 3. Special Unicode characters
        analyze_special_unicode()

        # 4. Mixed content
        analyze_mixed_content()

        # 5. Efficiency comparison
        compare_encoding_efficiency()

        # 6. Interactive mode
        print(f"\n{'='*70}")
        choice = (
            input("Want to try the interactive Unicode tokenizer? (y/n): ")
            .lower()
            .strip()
        )
        if choice == "y":
            interactive_unicode_tokenizer()

    except ImportError as e:
        print(f"âŒ Missing dependency: {e}")
        print("ğŸ“¦ Install required packages:")
        print("   pip install tiktoken")
    except Exception as e:
        print(f"âŒ Error: {e}")


if __name__ == "__main__":
    main()
